# -*- coding: utf-8 -*-
"""stats201 project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YNGPe0j-8sLiJF1rkD1tohj4GrK1mQI4
"""

"""NYU CS (with csv directory)"""

# ============================================================
# NYU CS faculty directory â€” download photos + export CSV
# URL: https://cs.nyu.edu/dynamic/people/faculty/
# Output: images folder + CSV(name, profile_url, image_url, local_path)
# Works in Google Colab (dynamic site) via Playwright Async API.
# ============================================================

# 1) Install (run once per runtime)
!pip -q install -U playwright
!playwright install --with-deps chromium

# 2) Mount Drive (optional but recommended)
from google.colab import drive
drive.mount("/content/drive")

import os, re, csv, hashlib, asyncio
from urllib.parse import urlparse
from playwright.async_api import async_playwright

START_URL = "https://cs.nyu.edu/dynamic/people/faculty/"
OUT_DIR  = "/content/drive/MyDrive/prof_photos/nyu_cs_faculty"      # change if you want
CSV_PATH = "/content/drive/MyDrive/prof_photos/nyu_cs_faculty.csv"  # change if you want

SCROLL_TIMES = 18          # increase if not all people load
WAIT_MS_AFTER_SCROLL = 600 # adjust if page loads slowly
MAX_PEOPLE = None          # e.g. 300; keep None for all found

os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)

def safe_filename(s: str) -> str:
    s = re.sub(r"[^a-zA-Z0-9._-]+", "_", (s or "").strip())
    return s[:180] if s else hashlib.md5(os.urandom(16)).hexdigest()

def ext_from_url(u: str) -> str:
    ext = os.path.splitext(urlparse(u).path)[1].lower()
    return ext if ext in [".jpg", ".jpeg", ".png", ".webp"] else ".jpg"

async def scrape_nyu_faculty_with_photos():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--single-process",
                "--no-zygote",
            ],
        )
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
        )
        page = await context.new_page()

        print("[info] goto:", START_URL)
        await page.goto(START_URL, wait_until="domcontentloaded", timeout=60000)
        await page.wait_for_timeout(2000)

        # Scroll to trigger lazy load / pagination
        for _ in range(SCROLL_TIMES):
            await page.mouse.wheel(0, 2400)
            await page.wait_for_timeout(WAIT_MS_AFTER_SCROLL)

        # Extract records from rendered DOM
        records = await page.evaluate("""
        () => {
          const base = location.href;
          const abs = (u) => {
            try { return new URL(u, base).href; } catch(e) { return null; }
          };

          // Helper: pick a "name-like" text from anchors within a container
          function pickNameAndProfile(container) {
            const links = Array.from(container.querySelectorAll("a[href]"))
              .map(a => ({ text: (a.textContent||"").trim(), href: abs(a.getAttribute("href")) }))
              .filter(x => x.href && x.text && x.text.toLowerCase() !== "image");

            // Prefer 2+ words (looks like a person name)
            const nameLike = links.find(x => x.text.split(/\\s+/).length >= 2) || links[0];
            return nameLike ? { name: nameLike.text, profile_url: nameLike.href } : { name: null, profile_url: null };
          }

          // Strategy A: "Image" anchors or direct faculty photo links
          const photoAnchors = Array.from(document.querySelectorAll("a[href]"))
            .filter(a => (a.getAttribute("href") || "").includes("/media/faculty_photos/"));

          // Strategy B: <img> tags that point to faculty_photos (or srcset)
          const photoImgs = Array.from(document.querySelectorAll("img"))
            .map(img => {
              const src = img.currentSrc || img.src || "";
              const srcset = img.getAttribute("srcset") || "";
              let best = src;
              if (!best && srcset) {
                const parts = srcset.split(",").map(s => s.trim().split(" ")[0]).filter(Boolean);
                if (parts.length) best = parts[parts.length - 1];
              }
              return { el: img, url: best };
            })
            .filter(x => (x.url || "").includes("/media/faculty_photos/"));

          const out = [];

          // From anchors
          for (const a of photoAnchors) {
            const image_url = abs(a.getAttribute("href"));
            if (!image_url) continue;

            const container = a.closest("li, article, div") || a.parentElement || document.body;
            const { name, profile_url } = pickNameAndProfile(container);

            out.push({ name, profile_url, image_url });
          }

          // From imgs
          for (const x of photoImgs) {
            const image_url = abs(x.url);
            if (!image_url) continue;

            const container = x.el.closest("li, article, div") || x.el.parentElement || document.body;
            const { name, profile_url } = pickNameAndProfile(container);

            out.push({ name, profile_url, image_url });
          }

          // De-duplicate by image_url; keep first non-null name/profile if possible
          const byImg = new Map();
          for (const r of out) {
            if (!r.image_url) continue;
            if (!byImg.has(r.image_url)) byImg.set(r.image_url, r);
            else {
              const cur = byImg.get(r.image_url);
              if ((!cur.name && r.name) || (!cur.profile_url && r.profile_url)) {
                byImg.set(r.image_url, { ...cur, ...r });
              }
            }
          }

          return Array.from(byImg.values());
        }
        """)

        print("[info] records found:", len(records))
        if len(records) == 0:
            await context.close()
            await browser.close()
            raise RuntimeError("No records found. The DOM structure may have changed; need a more specific selector.")

        if MAX_PEOPLE is not None:
            records = records[:MAX_PEOPLE]

        # Download images + write rows
        rows = []
        saved = 0
        for i, r in enumerate(records, 1):
            name = (r.get("name") or f"unknown_{i}").strip()
            profile_url = r.get("profile_url")
            image_url = r.get("image_url")

            filename = safe_filename(name) + ext_from_url(image_url)
            local_path = os.path.join(OUT_DIR, filename)

            if not os.path.exists(local_path):
                try:
                    resp = await context.request.get(image_url, timeout=60000)
                    if resp.ok:
                        ctype = (resp.headers.get("content-type") or "").lower()
                        if "image" in ctype:
                            data = await resp.body()
                            with open(local_path, "wb") as f:
                                f.write(data)
                            saved += 1
                        else:
                            local_path = None
                    else:
                        local_path = None
                except Exception:
                    local_path = None

            rows.append({
                "name": name,
                "profile_url": profile_url,
                "image_url": image_url,
                "local_path": local_path
            })

        # Write CSV
        with open(CSV_PATH, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["name", "profile_url", "image_url", "local_path"])
            w.writeheader()
            w.writerows(rows)

        await context.close()
        await browser.close()

        print(f"[done] saved images: {saved}/{len(rows)}")
        print("CSV:", CSV_PATH)
        print("Images:", OUT_DIR)

        return rows

rows = await scrape_nyu_faculty_with_photos()
print("preview (first 5):")
for r in rows[:5]:
    print(r["name"], "|", r["image_url"])

"""NYU Art (with csv directory)"""

import os, re, csv, hashlib
from urllib.parse import urlparse
from playwright.async_api import async_playwright

START_URL = "https://tisch.nyu.edu/drama/faculty/ft-faculty"
OUT_DIR  = "/content/drive/MyDrive/prof_photos/nyu_tisch_drama_ft"     
CSV_PATH = "/content/drive/MyDrive/prof_photos/nyu_tisch_drama_ft.csv" 

os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)

def safe_filename(s: str) -> str:
    s = re.sub(r"[^a-zA-Z0-9._-]+", "_", (s or "").strip())
    return s[:180] if s else hashlib.md5(os.urandom(16)).hexdigest()

def guess_ext_from_url(u: str) -> str:
    ext = os.path.splitext(urlparse(u).path)[1].lower()
    return ext if ext in [".jpg",".jpeg",".png",".webp"] else ".jpg"

async def scrape_and_download():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--single-process",
                "--no-zygote",
            ],
        )
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
        )
        page = await context.new_page()

        
        await page.goto(START_URL, wait_until="domcontentloaded", timeout=60000)
        await page.wait_for_timeout(2500)

        try:
            await page.wait_for_function("document.images.length > 5", timeout=20000)
        except Exception:
            pass


        records = await page.evaluate("""
        () => {
          const base = location.href;
          const abs = (u) => { try { return new URL(u, base).href; } catch(e) { return null; } };

          function getImgUrl(img) {
            let u = img.currentSrc || img.src || "";
            if (!u) {
              const srcset = img.getAttribute("srcset") || "";
              if (srcset) {
                const parts = srcset.split(",").map(s => s.trim().split(" ")[0]).filter(Boolean);
                if (parts.length) u = parts[parts.length - 1];
              }
            }
            return u ? abs(u) : null;
          }

          function looksLikeName(t) {
            if (!t) return false;
            t = t.trim();
            if (!/[A-Za-z]/.test(t)) return false;
            const words = t.split(/\\s+/);
            if (words.length < 2) return false;
            const bad = ["learn", "more", "view", "faculty", "staff", "department", "program", "contact", "email"];
            const low = t.toLowerCase();
            if (bad.some(b => low === b || low.startsWith(b + " "))) return false;
            return true;
          }

          function pickNameAndProfile(container) {
            if (!container) return { name: null, profile_url: null };

            const headings = Array.from(container.querySelectorAll("h1,h2,h3,h4,h5"))
              .map(h => (h.textContent || "").trim())
              .filter(looksLikeName);
            if (headings.length) {
              const h = container.querySelector("h1 a[href],h2 a[href],h3 a[href],h4 a[href],h5 a[href]");
              return { name: headings[0], profile_url: h ? abs(h.getAttribute("href")) : null };
            }

            const links = Array.from(container.querySelectorAll("a[href]"))
              .map(a => ({
                text: (a.textContent || "").trim(),
                href: abs(a.getAttribute("href"))
              }))
              .filter(x => x.href && looksLikeName(x.text));

            if (links.length) return { name: links[0].text, profile_url: links[0].href };

            const img = container.querySelector("img");
            const alt = img ? (img.alt || "").trim() : "";
            return looksLikeName(alt) ? { name: alt, profile_url: null } : { name: null, profile_url: null };
          }

          const out = [];

          const imgs = Array.from(document.querySelectorAll("img"));
          for (const img of imgs) {
            const w = img.naturalWidth || 0;
            const h = img.naturalHeight || 0;
            if (w && h && (w < 120 || h < 120)) continue;

            const image_url = getImgUrl(img);
            if (!image_url) continue;

            const low = image_url.toLowerCase();
            if (low.includes("logo") || low.includes("icon") || low.includes("sprite") || low.includes("banner")) continue;
            if (low.startsWith("data:image")) continue;

            const container = img.closest("article, li, section, div") || img.parentElement;
            const { name, profile_url } = pickNameAndProfile(container);

            out.push({ name, profile_url, image_url });
          }

          const map = new Map();
          for (const r of out) {
            if (!r.image_url) continue;
            if (!map.has(r.image_url)) {
              map.set(r.image_url, r);
            } else {
              const cur = map.get(r.image_url);
              const better = {
                name: cur.name || r.name,
                profile_url: cur.profile_url || r.profile_url,
                image_url: cur.image_url
              };
              map.set(r.image_url, better);
            }
          }

          const res = Array.from(map.values()).filter(r => r.name && r.name.trim().length >= 3);
          return res;
        }
        """)

        print("records found:", len(records))
        if len(records) == 0:
            await context.close()
            await browser.close()
            raise RuntimeError("No records found. The page structure may have changed; need different selectors.")

        rows = []
        saved = 0

        for i, r in enumerate(records, 1):
            name = (r.get("name") or f"unknown_{i}").strip()
            profile_url = r.get("profile_url")
            image_url = r.get("image_url")

            ext = guess_ext_from_url(image_url)
            filename = safe_filename(name) + ext
            local_path = os.path.join(OUT_DIR, filename)

            if not os.path.exists(local_path):
                try:
                    resp = await context.request.get(image_url, timeout=60000)
                    if resp.ok:
                        ctype = (resp.headers.get("content-type") or "").lower()
                        if "image" in ctype:
                            data = await resp.body()
                            with open(local_path, "wb") as f:
                                f.write(data)
                            saved += 1
                        else:
                            local_path = None
                    else:
                        local_path = None
                except Exception:
                    local_path = None

            rows.append({
                "name": name,
                "profile_url": profile_url,
                "image_url": image_url,
                "local_path": local_path
            })

        with open(CSV_PATH, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["name", "profile_url", "image_url", "local_path"])
            w.writeheader()
            w.writerows(rows)

        await context.close()
        await browser.close()

        print(f"saved images: {saved}/{len(rows)}")
        print("CSV:", CSV_PATH)
        print("Images:", OUT_DIR)

        return rows

rows = await scrape_and_download()
print("preview:", rows[:5])

"""UCLA CS (with csv directory)"""

# ============================
# UCLA Samueli faculty (CS) scraper
# URL: https://samueli.ucla.edu/search-faculty/#cs
# Output: images + CSV(name, profile_url, image_url, local_path)
# ============================

# ---- 0) Install Playwright (Colab run once) ----
!pip -q install -U playwright
!playwright install --with-deps chromium

# ---- 1) (Optional) Mount Google Drive ----
from google.colab import drive
drive.mount("/content/drive")

import os, re, csv, hashlib
from urllib.parse import urlparse
from playwright.async_api import async_playwright

START_URL = "https://samueli.ucla.edu/search-faculty/#cs"

# Change these paths as you like
OUT_DIR  = "/content/drive/MyDrive/prof_photos/ucla_samueli_cs"
CSV_PATH = "/content/drive/MyDrive/prof_photos/ucla_samueli_cs_faculty.csv"

DOWNLOAD_IMAGES = True          
SCROLL_TIMES = 25               
WAIT_MS_AFTER_SCROLL = 700
MAX_PEOPLE = None              

os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)

def safe_filename(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"[^a-zA-Z0-9._ -]+", "", s)
    s = s.replace(" ", "_")
    return s[:160] if s else hashlib.md5(os.urandom(16)).hexdigest()

def ext_from_url(u: str) -> str:
    ext = os.path.splitext(urlparse(u).path)[1].lower()
    return ext if ext in [".jpg", ".jpeg", ".png", ".webp"] else ".jpg"

async def scrape_ucla_cs():
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--single-process",
                "--no-zygote",
            ],
        )
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            )
        )
        page = await context.new_page()

        print("[info] goto:", START_URL)
        await page.goto(START_URL, wait_until="domcontentloaded", timeout=60000)

        
        await page.wait_for_timeout(3000)

        try:
            await page.wait_for_function(
                """() => {
                    const hasPeopleLinks = document.querySelectorAll('a[href*="/people/"], a[href*="/faculty/"], a[href*="/directory/"]').length > 5;
                    const hasImgs = document.images.length > 10;
                    return hasPeopleLinks || hasImgs;
                }""",
                timeout=20000
            )
        except Exception:
            pass

        for _ in range(SCROLL_TIMES):
            await page.mouse.wheel(0, 2400)
            await page.wait_for_timeout(WAIT_MS_AFTER_SCROLL)

        # ---- Extract records from rendered DOM ----
        records = await page.evaluate("""
        () => {
          const base = location.href;
          const abs = (u) => { try { return new URL(u, base).href; } catch(e) { return null; } };

          const badImgKw = ["logo","icon","sprite","banner","loading","spinner"];
          const looksLikeName = (t) => {
            if (!t) return false;
            t = t.trim();
            if (!/[A-Za-z]/.test(t)) return false;
            const words = t.split(/\\s+/);
            if (words.length < 2) return false;
            const low = t.toLowerCase();
            const bad = ["learn more","view profile","profile","faculty","staff","department","research"];
            if (bad.includes(low)) return false;
            return true;
          };

          const pickNameProfile = (container) => {
            if (!container) return {name:null, profile_url:null};

            // headings first
            const hs = Array.from(container.querySelectorAll("h1,h2,h3,h4,h5"))
              .map(h => (h.textContent||"").trim())
              .filter(looksLikeName);
            if (hs.length) {
              const a = container.querySelector("h1 a[href],h2 a[href],h3 a[href],h4 a[href],h5 a[href]");
              return { name: hs[0], profile_url: a ? abs(a.getAttribute("href")) : null };
            }

            // then name-like links
            const links = Array.from(container.querySelectorAll("a[href]"))
              .map(a => ({ text:(a.textContent||"").trim(), href: abs(a.getAttribute("href")||"") }))
              .filter(x => x.href && looksLikeName(x.text));
            if (links.length) return { name: links[0].text, profile_url: links[0].href };

            // alt fallback
            const img = container.querySelector("img");
            const alt = img ? (img.alt||"").trim() : "";
            return looksLikeName(alt) ? { name: alt, profile_url: null } : { name: null, profile_url: null };
          };

          const getImgUrl = (img) => {
            let u = img.currentSrc || img.src || "";
            if (!u) {
              const srcset = img.getAttribute("srcset") || "";
              if (srcset) {
                const parts = srcset.split(",").map(s => s.trim().split(" ")[0]).filter(Boolean);
                if (parts.length) u = parts[parts.length - 1];
              }
            }
            return u ? abs(u) : null;
          };

          const out = [];

          // Use images as anchors (most stable)
          const imgs = Array.from(document.querySelectorAll("img"));
          for (const img of imgs) {
            const image_url = getImgUrl(img);
            if (!image_url) continue;

            const low = image_url.toLowerCase();
            if (low.startsWith("data:image")) continue;
            if (badImgKw.some(k => low.includes(k))) continue;

            const w = img.naturalWidth || 0;
            const h = img.naturalHeight || 0;
            if (w && h && (w < 120 || h < 120)) continue;

            // find container around image
            const container = img.closest("article, li, section, div") || img.parentElement;

            // Heuristic: keep containers likely representing a person card
            // If the container has a profile link to samueli.ucla.edu, treat as candidate
            const hasProfileLink = container && container.querySelector('a[href*="samueli.ucla.edu/people/"],a[href*="samueli.ucla.edu/faculty/"],a[href*="samueli.ucla.edu/directory/"]');
            if (!hasProfileLink) continue;

            const { name, profile_url } = pickNameProfile(container);
            out.push({ name, profile_url, image_url });
          }

          // Dedup by image_url (prefer rows with name)
          const map = new Map();
          for (const r of out) {
            if (!r.image_url) continue;
            if (!map.has(r.image_url)) map.set(r.image_url, r);
            else {
              const cur = map.get(r.image_url);
              if (!cur.name && r.name) map.set(r.image_url, r);
            }
          }

          // Filter: require name (to avoid random images)
          return Array.from(map.values()).filter(r => r.name && r.name.trim().length >= 3);
        }
        """)

        print("[info] records found:", len(records))
        if len(records) == 0:
            await context.close()
            await browser.close()
            raise RuntimeError("No records found. Page structure may have changed; need a more specific selector.")

        if MAX_PEOPLE is not None:
            records = records[:MAX_PEOPLE]

        # ---- Download images (optional) + build CSV rows ----
        rows = []
        saved = 0

        for i, r in enumerate(records, 1):
            name = (r.get("name") or f"unknown_{i}").strip()
            profile_url = r.get("profile_url")
            image_url = r.get("image_url")

            local_path = None
            if DOWNLOAD_IMAGES:
                ext = ext_from_url(image_url)
                filename = safe_filename(name) + ext
                local_path = os.path.join(OUT_DIR, filename)

                if not os.path.exists(local_path):
                    try:
                        resp = await context.request.get(image_url, timeout=60000)
                        if resp.ok:
                            ctype = (resp.headers.get("content-type") or "").lower()
                            if "image" in ctype:
                                data = await resp.body()
                                with open(local_path, "wb") as f:
                                    f.write(data)
                                saved += 1
                            else:
                                local_path = None
                        else:
                            local_path = None
                    except Exception:
                        local_path = None

            rows.append({
                "name": name,
                "profile_url": profile_url,
                "image_url": image_url,
                "local_path": local_path
            })

        # ---- Write CSV ----
        with open(CSV_PATH, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=["name", "profile_url", "image_url", "local_path"])
            w.writeheader()
            w.writerows(rows)

        await context.close()
        await browser.close()

        print(f"[done] saved images: {saved}/{len(rows)}" if DOWNLOAD_IMAGES else "[done] images download skipped")
        print("[done] CSV:", CSV_PATH)
        print("[done] Images:", OUT_DIR)

        return rows

rows = await scrape_ucla_cs()
print("preview:", rows[:5])

"""UCLA Art (with csv directory)"""

import os, re, csv, time
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

START_URL = "https://www.art.ucla.edu/faculty/"

OUT_DIR  = "/content/drive/MyDrive/prof_photos/ucla_art_faculty_photos"
CSV_PATH = "/content/drive/MyDrive/prof_photos/ucla_art_faculty_photos.csv"

os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; research-bot/1.0; +contact: your_email@example.com)"
}

def safe_filename(name: str, max_len: int = 140) -> str:
    name = (name or "").strip()
    name = re.sub(r"\s+", " ", name)
    name = re.sub(r"[^a-zA-Z0-9._ -]+", "", name)
    name = name.replace(" ", "_")
    return name[:max_len] if name else "unknown"

def ext_from_content_type(ct: str) -> str:
    ct = (ct or "").lower()
    if "png" in ct: return ".png"
    if "webp" in ct: return ".webp"
    if "jpeg" in ct or "jpg" in ct: return ".jpg"
    return ".jpg"

def unique_path(out_dir: str, base: str, ext: str) -> str:
    path = os.path.join(out_dir, f"{base}{ext}")
    k = 2
    while os.path.exists(path):
        path = os.path.join(out_dir, f"{base}_{k}{ext}")
        k += 1
    return path

session = requests.Session()
session.headers.update(HEADERS)

resp = session.get(START_URL, timeout=30)
resp.raise_for_status()

soup = BeautifulSoup(resp.text, "html.parser")

bad_alt = {
    "UCLA Department of Art", "UCLA", "Logo", "logo", "Search", "menu"
}
bad_src_kw = ["logo", "icon", "sprite", "banner", "placeholder", "loading", "spinner"]

imgs = []
for img in soup.select("img"):
    alt = (img.get("alt") or "").strip()
    src = img.get("src") or img.get("data-src") or ""
    if not alt or alt in bad_alt:
        continue
    if len(alt) < 3:
        continue
    if not src:
        continue
    full = urljoin(START_URL, src)

    low = full.lower()
    if any(k in low for k in bad_src_kw):
        continue
    if low.startswith("data:image"):
        continue

    imgs.append({"name": alt, "image_url": full})

seen = set()
records = []
for it in imgs:
    if it["image_url"] in seen:
        continue
    seen.add(it["image_url"])
    records.append(it)

print("Found images:", len(records))
if len(records) == 0:
    raise RuntimeError("no image")

rows = []
saved = 0
for i, r in enumerate(records, 1):
    name = r["name"]
    image_url = r["image_url"]

    try:
        img_resp = session.get(image_url, timeout=30)
        if img_resp.status_code != 200:
            rows.append({"name": name, "image_url": image_url, "local_path": None, "group": "UCLA Art Faculty Page"})
            continue

        ctype = img_resp.headers.get("Content-Type", "")
        if "image" not in (ctype or "").lower():
            rows.append({"name": name, "image_url": image_url, "local_path": None, "group": "UCLA Art Faculty Page"})
            continue

        ext = ext_from_content_type(ctype)
        base = safe_filename(name)
        path = unique_path(OUT_DIR, base, ext)

        with open(path, "wb") as f:
            f.write(img_resp.content)

        saved += 1
        rows.append({"name": name, "image_url": image_url, "local_path": path, "group": "UCLA Art Faculty Page"})

        time.sleep(0.4)  
    except Exception:
        rows.append({"name": name, "image_url": image_url, "local_path": None, "group": "UCLA Art Faculty Page"})

with open(CSV_PATH, "w", newline="", encoding="utf-8-sig") as f:
    w = csv.DictWriter(f, fieldnames=["name", "image_url", "local_path", "group"])
    w.writeheader()
    w.writerows(rows)

print(f"Saved images: {saved}/{len(rows)}")
print("Images dir:", OUT_DIR)
print("CSV:", CSV_PATH)
